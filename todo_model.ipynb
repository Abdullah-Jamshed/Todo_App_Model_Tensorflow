{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "# import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sum</th>\n",
       "      <th>bread</th>\n",
       "      <th>egg</th>\n",
       "      <th>waffles</th>\n",
       "      <th>butter</th>\n",
       "      <th>milk</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>...</th>\n",
       "      <th>strawberry</th>\n",
       "      <th>peach</th>\n",
       "      <th>coriander</th>\n",
       "      <th>mint</th>\n",
       "      <th>tomato</th>\n",
       "      <th>onion</th>\n",
       "      <th>potatoes</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ginger</th>\n",
       "      <th>pepper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>5118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            day  month  year   sum  bread  egg  waffles  butter  milk  yogurt  \\\n",
       "Date                                                                            \n",
       "2018-01-01    1      1    18  1118      1    1        1       1     1       1   \n",
       "2018-01-02    2      1    18  2118      1    0        0       0     1       1   \n",
       "2018-01-03    3      1    18  3118      0    0        0       0     1       1   \n",
       "2018-01-04    4      1    18  4118      1    1        0       0     1       0   \n",
       "2018-01-05    5      1    18  5118      1    1        0       0     1       1   \n",
       "\n",
       "            ...  strawberry  peach  coriander  mint  tomato  onion  potatoes  \\\n",
       "Date        ...                                                                \n",
       "2018-01-01  ...           0      1          1     0       1      1         1   \n",
       "2018-01-02  ...           0      0          1     0       0      0         0   \n",
       "2018-01-03  ...           0      0          0     0       1      0         0   \n",
       "2018-01-04  ...           0      0          1     1       1      0         0   \n",
       "2018-01-05  ...           1      1          1     0       1      0         0   \n",
       "\n",
       "            garlic  ginger  pepper  \n",
       "Date                                \n",
       "2018-01-01       1       1       0  \n",
       "2018-01-02       0       0       0  \n",
       "2018-01-03       0       0       0  \n",
       "2018-01-04       0       0       0  \n",
       "2018-01-05       1       1       1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('todoData.xlsx', date_parser=True)\n",
    "data = data.set_index('Date')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sum</th>\n",
       "      <th>bread</th>\n",
       "      <th>egg</th>\n",
       "      <th>waffles</th>\n",
       "      <th>butter</th>\n",
       "      <th>milk</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>...</th>\n",
       "      <th>strawberry</th>\n",
       "      <th>peach</th>\n",
       "      <th>coriander</th>\n",
       "      <th>mint</th>\n",
       "      <th>tomato</th>\n",
       "      <th>onion</th>\n",
       "      <th>potatoes</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ginger</th>\n",
       "      <th>pepper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.720548</td>\n",
       "      <td>6.526027</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>52317.678082</td>\n",
       "      <td>0.617808</td>\n",
       "      <td>0.516438</td>\n",
       "      <td>0.108219</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.867123</td>\n",
       "      <td>0.735616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.398630</td>\n",
       "      <td>0.667123</td>\n",
       "      <td>0.249315</td>\n",
       "      <td>0.832877</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>0.250685</td>\n",
       "      <td>0.250685</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.802278</td>\n",
       "      <td>3.450215</td>\n",
       "      <td>0.500343</td>\n",
       "      <td>76892.181651</td>\n",
       "      <td>0.486256</td>\n",
       "      <td>0.500072</td>\n",
       "      <td>0.310870</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.339674</td>\n",
       "      <td>0.441307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400274</td>\n",
       "      <td>0.489952</td>\n",
       "      <td>0.471566</td>\n",
       "      <td>0.432913</td>\n",
       "      <td>0.373342</td>\n",
       "      <td>0.400274</td>\n",
       "      <td>0.272748</td>\n",
       "      <td>0.433705</td>\n",
       "      <td>0.433705</td>\n",
       "      <td>0.400274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1118.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>11118.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>20918.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>30918.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>311219.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              day       month        year            sum       bread  \\\n",
       "count  730.000000  730.000000  730.000000     730.000000  730.000000   \n",
       "mean    15.720548    6.526027   18.500000   52317.678082    0.617808   \n",
       "std      8.802278    3.450215    0.500343   76892.181651    0.486256   \n",
       "min      1.000000    1.000000   18.000000    1118.000000    0.000000   \n",
       "25%      8.000000    4.000000   18.000000   11118.000000    0.000000   \n",
       "50%     16.000000    7.000000   18.500000   20918.500000    1.000000   \n",
       "75%     23.000000   10.000000   19.000000   30918.750000    1.000000   \n",
       "max     31.000000   12.000000   19.000000  311219.000000    1.000000   \n",
       "\n",
       "              egg     waffles      butter        milk      yogurt  ...  \\\n",
       "count  730.000000  730.000000  730.000000  730.000000  730.000000  ...   \n",
       "mean     0.516438    0.108219    0.321918    0.867123    0.735616  ...   \n",
       "std      0.500072    0.310870    0.467532    0.339674    0.441307  ...   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
       "25%      0.000000    0.000000    0.000000    1.000000    0.000000  ...   \n",
       "50%      1.000000    0.000000    0.000000    1.000000    1.000000  ...   \n",
       "75%      1.000000    0.000000    1.000000    1.000000    1.000000  ...   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000  ...   \n",
       "\n",
       "       strawberry       peach   coriander        mint      tomato       onion  \\\n",
       "count  730.000000  730.000000  730.000000  730.000000  730.000000  730.000000   \n",
       "mean     0.200000    0.398630    0.667123    0.249315    0.832877    0.200000   \n",
       "std      0.400274    0.489952    0.471566    0.432913    0.373342    0.400274   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000    0.000000    1.000000    0.000000   \n",
       "75%      0.000000    1.000000    1.000000    0.000000    1.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         potatoes      garlic      ginger      pepper  \n",
       "count  730.000000  730.000000  730.000000  730.000000  \n",
       "mean     0.080822    0.250685    0.250685    0.200000  \n",
       "std      0.272748    0.433705    0.433705    0.400274  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.750000    0.750000    0.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Training, Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ,test= data.iloc[:-100,3:] , data.iloc[-100:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on 3 days then predciting 4th value\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range (3, train.shape[0]):\n",
    "    x_train.append(train[i-3:i])\n",
    "    y_train.append(train[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling traing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 3, 30)\n",
      "(627, 29)\n"
     ]
    }
   ],
   "source": [
    "#converting traning data into munpy array\n",
    "\n",
    "x_train , y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "y_train = y_train[:,1:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# building LSTM network\n",
    "\n",
    "numbers_of_features = 30\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 140, activation = \"relu\", return_sequences = True, input_shape= (x_train.shape[1],30)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 120, activation = \"relu\", return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 100, activation = \"relu\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 80, activation = \"relu\"))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(Dense(29, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 3, 140)            95760     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 140)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 3, 120)            125280    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 120)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 3, 100)            88400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 80)                57920     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 29)                2349      \n",
      "=================================================================\n",
      "Total params: 369,709\n",
      "Trainable params: 369,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "627/627 [==============================] - 7s 12ms/sample - loss: 0.6034 - acc: 0.7518\n",
      "Epoch 2/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.5077 - acc: 0.7717\n",
      "Epoch 3/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4967 - acc: 0.7739\n",
      "Epoch 4/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4926 - acc: 0.7750\n",
      "Epoch 5/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4892 - acc: 0.7756\n",
      "Epoch 6/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4807 - acc: 0.7802\n",
      "Epoch 7/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4742 - acc: 0.7847\n",
      "Epoch 8/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4645 - acc: 0.7894\n",
      "Epoch 9/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4564 - acc: 0.7942\n",
      "Epoch 10/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4409 - acc: 0.8012\n",
      "Epoch 11/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.4155 - acc: 0.8151\n",
      "Epoch 12/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3921 - acc: 0.8208\n",
      "Epoch 13/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3748 - acc: 0.8312\n",
      "Epoch 14/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3569 - acc: 0.8455\n",
      "Epoch 15/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3382 - acc: 0.8565\n",
      "Epoch 16/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3228 - acc: 0.8647 1s - loss\n",
      "Epoch 17/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3157 - acc: 0.8704\n",
      "Epoch 18/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.3005 - acc: 0.8788\n",
      "Epoch 19/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2890 - acc: 0.8810\n",
      "Epoch 20/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2835 - acc: 0.8830\n",
      "Epoch 21/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2760 - acc: 0.8869\n",
      "Epoch 22/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2688 - acc: 0.8878\n",
      "Epoch 23/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2660 - acc: 0.8905\n",
      "Epoch 24/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2635 - acc: 0.8920\n",
      "Epoch 25/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2581 - acc: 0.8930 1s - loss: 0.2521 - acc: 0.89 - ETA: 1s - loss: 0.2548  - ETA: 0s - loss: 0.2598 - acc: \n",
      "Epoch 26/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2526 - acc: 0.8961\n",
      "Epoch 27/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2535 - acc: 0.8968\n",
      "Epoch 28/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2484 - acc: 0.8968\n",
      "Epoch 29/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2457 - acc: 0.8990\n",
      "Epoch 30/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2443 - acc: 0.8996\n",
      "Epoch 31/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2363 - acc: 0.9044 1s - loss: 0\n",
      "Epoch 32/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2358 - acc: 0.9045\n",
      "Epoch 33/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2337 - acc: 0.9038\n",
      "Epoch 34/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2372 - acc: 0.9040\n",
      "Epoch 35/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2290 - acc: 0.9071\n",
      "Epoch 36/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2257 - acc: 0.9065\n",
      "Epoch 37/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2277 - acc: 0.9055\n",
      "Epoch 38/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2246 - acc: 0.9083\n",
      "Epoch 39/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2202 - acc: 0.9106\n",
      "Epoch 40/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2207 - acc: 0.9085\n",
      "Epoch 41/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2160 - acc: 0.9106\n",
      "Epoch 42/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2170 - acc: 0.9093\n",
      "Epoch 43/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2169 - acc: 0.9104\n",
      "Epoch 44/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2138 - acc: 0.9124\n",
      "Epoch 45/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2156 - acc: 0.9112\n",
      "Epoch 46/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2128 - acc: 0.9106\n",
      "Epoch 47/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2119 - acc: 0.9102\n",
      "Epoch 48/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2091 - acc: 0.9125\n",
      "Epoch 49/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2061 - acc: 0.9128\n",
      "Epoch 50/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2045 - acc: 0.9148\n",
      "Epoch 51/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2039 - acc: 0.9144\n",
      "Epoch 52/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2031 - acc: 0.9141\n",
      "Epoch 53/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2071 - acc: 0.9124\n",
      "Epoch 54/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2002 - acc: 0.9149\n",
      "Epoch 55/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2000 - acc: 0.9161\n",
      "Epoch 56/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2029 - acc: 0.9149\n",
      "Epoch 57/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2049 - acc: 0.9139\n",
      "Epoch 58/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2038 - acc: 0.9129\n",
      "Epoch 59/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.2067 - acc: 0.9116\n",
      "Epoch 60/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1969 - acc: 0.9155\n",
      "Epoch 61/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1940 - acc: 0.9175\n",
      "Epoch 62/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1932 - acc: 0.9177\n",
      "Epoch 63/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1941 - acc: 0.9189\n",
      "Epoch 64/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1929 - acc: 0.9176\n",
      "Epoch 65/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1867 - acc: 0.9194\n",
      "Epoch 66/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1899 - acc: 0.9186\n",
      "Epoch 67/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1898 - acc: 0.9179\n",
      "Epoch 68/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1893 - acc: 0.9177\n",
      "Epoch 69/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1880 - acc: 0.9194\n",
      "Epoch 70/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1846 - acc: 0.9203\n",
      "Epoch 71/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1854 - acc: 0.9189\n",
      "Epoch 72/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1855 - acc: 0.9201\n",
      "Epoch 73/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1876 - acc: 0.9178\n",
      "Epoch 74/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1850 - acc: 0.9207\n",
      "Epoch 75/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1820 - acc: 0.9208\n",
      "Epoch 76/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1815 - acc: 0.9212\n",
      "Epoch 77/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1784 - acc: 0.9232\n",
      "Epoch 78/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1800 - acc: 0.9232\n",
      "Epoch 79/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1799 - acc: 0.9208 0s - loss: 0.1802 - acc: 0.9\n",
      "Epoch 80/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1923 - acc: 0.9187\n",
      "Epoch 81/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1848 - acc: 0.9210\n",
      "Epoch 82/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1799 - acc: 0.9222\n",
      "Epoch 83/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1785 - acc: 0.9226\n",
      "Epoch 84/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1860 - acc: 0.9198\n",
      "Epoch 85/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1800 - acc: 0.9218\n",
      "Epoch 86/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1750 - acc: 0.9238\n",
      "Epoch 87/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1750 - acc: 0.9257\n",
      "Epoch 88/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1776 - acc: 0.9243\n",
      "Epoch 89/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1747 - acc: 0.9232\n",
      "Epoch 90/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1728 - acc: 0.9234\n",
      "Epoch 91/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1725 - acc: 0.9251\n",
      "Epoch 92/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1701 - acc: 0.9265\n",
      "Epoch 93/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1765 - acc: 0.9231\n",
      "Epoch 94/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1686 - acc: 0.9273\n",
      "Epoch 95/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1639 - acc: 0.9283\n",
      "Epoch 96/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1685 - acc: 0.9265\n",
      "Epoch 97/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1664 - acc: 0.9278\n",
      "Epoch 98/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1643 - acc: 0.9292\n",
      "Epoch 99/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1657 - acc: 0.9295\n",
      "Epoch 100/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1620 - acc: 0.9299\n",
      "Epoch 101/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1588 - acc: 0.9295\n",
      "Epoch 102/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1639 - acc: 0.9280\n",
      "Epoch 103/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1686 - acc: 0.9245\n",
      "Epoch 104/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1621 - acc: 0.9305\n",
      "Epoch 105/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1602 - acc: 0.9302\n",
      "Epoch 106/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1611 - acc: 0.9302\n",
      "Epoch 107/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1594 - acc: 0.9304\n",
      "Epoch 108/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1565 - acc: 0.9310\n",
      "Epoch 109/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1547 - acc: 0.9311\n",
      "Epoch 110/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1520 - acc: 0.9333\n",
      "Epoch 111/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1494 - acc: 0.9327\n",
      "Epoch 112/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1482 - acc: 0.9332\n",
      "Epoch 113/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1582 - acc: 0.9310\n",
      "Epoch 114/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1514 - acc: 0.9325\n",
      "Epoch 115/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1489 - acc: 0.9338\n",
      "Epoch 116/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1508 - acc: 0.9337\n",
      "Epoch 117/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1479 - acc: 0.9365\n",
      "Epoch 118/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1534 - acc: 0.9312 0s - loss: 0.1530 - ac\n",
      "Epoch 119/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1541 - acc: 0.9327\n",
      "Epoch 120/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1457 - acc: 0.9368\n",
      "Epoch 121/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1465 - acc: 0.9345\n",
      "Epoch 122/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1413 - acc: 0.9355\n",
      "Epoch 123/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1420 - acc: 0.9368\n",
      "Epoch 124/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1518 - acc: 0.9342\n",
      "Epoch 125/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1533 - acc: 0.9334\n",
      "Epoch 126/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1456 - acc: 0.9338\n",
      "Epoch 127/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1454 - acc: 0.9350\n",
      "Epoch 128/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1411 - acc: 0.9362 0s - loss: 0.1398 -\n",
      "Epoch 129/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1406 - acc: 0.9372\n",
      "Epoch 130/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1453 - acc: 0.9349\n",
      "Epoch 131/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1479 - acc: 0.9357\n",
      "Epoch 132/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1421 - acc: 0.9354\n",
      "Epoch 133/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1423 - acc: 0.9372\n",
      "Epoch 134/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1396 - acc: 0.9380\n",
      "Epoch 135/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1363 - acc: 0.9395\n",
      "Epoch 136/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1363 - acc: 0.9381\n",
      "Epoch 137/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1343 - acc: 0.9396\n",
      "Epoch 138/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1321 - acc: 0.9395\n",
      "Epoch 139/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1392 - acc: 0.9376\n",
      "Epoch 140/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1338 - acc: 0.9389\n",
      "Epoch 141/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1355 - acc: 0.9386\n",
      "Epoch 142/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1408 - acc: 0.9358\n",
      "Epoch 143/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1376 - acc: 0.9400\n",
      "Epoch 144/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1312 - acc: 0.9403\n",
      "Epoch 145/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1306 - acc: 0.9413\n",
      "Epoch 146/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1338 - acc: 0.9414\n",
      "Epoch 147/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1333 - acc: 0.9393\n",
      "Epoch 148/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1321 - acc: 0.9398\n",
      "Epoch 149/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1317 - acc: 0.9410\n",
      "Epoch 150/150\n",
      "627/627 [==============================] - 2s 3ms/sample - loss: 0.1333 - acc: 0.9398 1s - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25f92e7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compiling the regressor model\n",
    "\n",
    "regressor.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "#training the the model\n",
    "\n",
    "regressor.fit(x_train, y_train, epochs=150, batch_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 3, 30)\n",
      "(97, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\FC\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler.fit(test)\n",
    "test = scaler.fit_transform(test)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(3, test.shape[0]):\n",
    "    x_test.append(test[i-3:i])\n",
    "    y_test.append(test[i])\n",
    "    \n",
    "# converting traning data into munpy array\n",
    "\n",
    "x_test , y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "y_test = y_test[:,1:]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 9ms/sample - loss: 0.2499 - acc: 0.9307\n"
     ]
    }
   ],
   "source": [
    "# predicting item\n",
    "result = regressor.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor.save('todomodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
